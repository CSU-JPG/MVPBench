<!DOCTYPE html>
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete"><!-- Head Start --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="description" content="Region-level Multi-modal LLM">
  <meta property="og:title" content="Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT">
  <meta property="og:description" content="Region-level Multi-modal LLM">
  <meta property="og:url" content="">


  <meta name="twitter:title" content="Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT">
  <meta name="twitter:description" content="Region-level Multi-modal LLM">

  <title>Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT</title>
  <style type="text/css">svg:not(:root).svg-inline--fa{overflow:visible}.svg-inline--fa{display:inline-block;font-size:inherit;height:1em;overflow:visible;vertical-align:-.125em}.svg-inline--fa.fa-lg{vertical-align:-.225em}.svg-inline--fa.fa-w-1{width:.0625em}.svg-inline--fa.fa-w-2{width:.125em}.svg-inline--fa.fa-w-3{width:.1875em}.svg-inline--fa.fa-w-4{width:.25em}.svg-inline--fa.fa-w-5{width:.3125em}.svg-inline--fa.fa-w-6{width:.375em}.svg-inline--fa.fa-w-7{width:.4375em}.svg-inline--fa.fa-w-8{width:.5em}.svg-inline--fa.fa-w-9{width:.5625em}.svg-inline--fa.fa-w-10{width:.625em}.svg-inline--fa.fa-w-11{width:.6875em}.svg-inline--fa.fa-w-12{width:.75em}.svg-inline--fa.fa-w-13{width:.8125em}.svg-inline--fa.fa-w-14{width:.875em}.svg-inline--fa.fa-w-15{width:.9375em}.svg-inline--fa.fa-w-16{width:1em}.svg-inline--fa.fa-w-17{width:1.0625em}.svg-inline--fa.fa-w-18{width:1.125em}.svg-inline--fa.fa-w-19{width:1.1875em}.svg-inline--fa.fa-w-20{width:1.25em}.svg-inline--fa.fa-pull-left{margin-right:.3em;width:auto}.svg-inline--fa.fa-pull-right{margin-left:.3em;width:auto}.svg-inline--fa.fa-border{height:1.5em}.svg-inline--fa.fa-li{width:2em}.svg-inline--fa.fa-fw{width:1.25em}.fa-layers svg.svg-inline--fa{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.fa-layers{display:inline-block;height:1em;position:relative;text-align:center;vertical-align:-.125em;width:1em}.fa-layers svg.svg-inline--fa{-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter,.fa-layers-text{display:inline-block;position:absolute;text-align:center}.fa-layers-text{left:50%;top:50%;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%);-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter{background-color:#ff253a;border-radius:1em;-webkit-box-sizing:border-box;box-sizing:border-box;color:#fff;height:1.5em;line-height:1;max-width:5em;min-width:1.5em;overflow:hidden;padding:.25em;right:0;text-overflow:ellipsis;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-bottom-right{bottom:0;right:0;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom right;transform-origin:bottom right}.fa-layers-bottom-left{bottom:0;left:0;right:auto;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom left;transform-origin:bottom left}.fa-layers-top-right{right:0;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-top-left{left:0;right:auto;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top left;transform-origin:top left}.fa-lg{font-size:1.3333333333em;line-height:.75em;vertical-align:-.0667em}.fa-xs{font-size:.75em}.fa-sm{font-size:.875em}.fa-1x{font-size:1em}.fa-2x{font-size:2em}.fa-3x{font-size:3em}.fa-4x{font-size:4em}.fa-5x{font-size:5em}.fa-6x{font-size:6em}.fa-7x{font-size:7em}.fa-8x{font-size:8em}.fa-9x{font-size:9em}.fa-10x{font-size:10em}.fa-fw{text-align:center;width:1.25em}.fa-ul{list-style-type:none;margin-left:2.5em;padding-left:0}.fa-ul>li{position:relative}.fa-li{left:-2em;position:absolute;text-align:center;width:2em;line-height:inherit}.fa-border{border:solid .08em #eee;border-radius:.1em;padding:.2em .25em .15em}.fa-pull-left{float:left}.fa-pull-right{float:right}.fa.fa-pull-left,.fab.fa-pull-left,.fal.fa-pull-left,.far.fa-pull-left,.fas.fa-pull-left{margin-right:.3em}.fa.fa-pull-right,.fab.fa-pull-right,.fal.fa-pull-right,.far.fa-pull-right,.fas.fa-pull-right{margin-left:.3em}.fa-spin{-webkit-animation:fa-spin 2s infinite linear;animation:fa-spin 2s infinite linear}.fa-pulse{-webkit-animation:fa-spin 1s infinite steps(8);animation:fa-spin 1s infinite steps(8)}@-webkit-keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.fa-rotate-90{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.fa-rotate-180{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.fa-rotate-270{-webkit-transform:rotate(270deg);transform:rotate(270deg)}.fa-flip-horizontal{-webkit-transform:scale(-1,1);transform:scale(-1,1)}.fa-flip-vertical{-webkit-transform:scale(1,-1);transform:scale(1,-1)}.fa-flip-both,.fa-flip-horizontal.fa-flip-vertical{-webkit-transform:scale(-1,-1);transform:scale(-1,-1)}:root .fa-flip-both,:root .fa-flip-horizontal,:root .fa-flip-vertical,:root .fa-rotate-180,:root .fa-rotate-270,:root .fa-rotate-90{-webkit-filter:none;filter:none}.fa-stack{display:inline-block;height:2em;position:relative;width:2.5em}.fa-stack-1x,.fa-stack-2x{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.svg-inline--fa.fa-stack-1x{height:1em;width:1.25em}.svg-inline--fa.fa-stack-2x{height:2em;width:2.5em}.fa-inverse{color:#fff}.sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.sr-only-focusable:active,.sr-only-focusable:focus{clip:auto;height:auto;margin:0;overflow:visible;position:static;width:auto}.svg-inline--fa .fa-primary{fill:var(--fa-primary-color,currentColor);opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa .fa-secondary{fill:var(--fa-secondary-color,currentColor);opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-primary{opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-secondary{opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa mask .fa-primary,.svg-inline--fa mask .fa-secondary{fill:#000}.fad.fa-inverse{color:#fff}</style>
  <link href="./static/css/css" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/others.css">

  <script src="./static/js/jquery.min.js"></script>
  <script src="./static/js/main.js"></script>
  <script src="./static/js/ViewSDKInterface.js"></script>
  <script defer="" src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
<style type="text/css">.ViewSDK_hideOverflow {
        overflow: hidden;
    }</style><style type="text/css">.ViewSDK_parentRelativeWidth {
        width: 100%;
    }</style><style type="text/css">.ViewSDK_viewportRelativeWidth {
        width: 100vw;
    }</style><style type="text/css">.ViewSDK_parentRelativeHeight {
        height: 100%;
    }</style><style type="text/css">.ViewSDK_viewportRelativeHeight {
        height: 100vh;
    }</style><style type="text/css">.ViewSDK_fullScreenPDFViewer {
        width:100vw !important; height:100vh !important;
        max-width:100vw !important; max-height:100vh !important;
        position:fixed; top:0; left:0; z-index:99999;
        margin: 0 !important; padding: 0 !important;
        border: none !important;
    }</style><style type="text/css">:root { --ViewSDK-mobile-viewport-height:1vh; }</style><style type="text/css">.ViewSDK_fullScreenPDFViewerMobile {
        width:100vw !important; height:calc(100 * var(--ViewSDK-mobile-viewport-height)) !important;
        max-width:100vw !important; max-height:calc(100 * var(--ViewSDK-mobile-viewport-height)) !important;
        position:fixed; top:0; left:0; z-index:99999;
        margin: 0 !important; padding: 0 !important;
        border: none !important;
    }</style><style type="text/css">.ViewSDK_hideOverflow {
        overflow: hidden;
    }</style><style type="text/css">.ViewSDK_parentRelativeWidth {
        width: 100%;
    }</style><style type="text/css">.ViewSDK_viewportRelativeWidth {
        width: 100vw;
    }</style><style type="text/css">.ViewSDK_parentRelativeHeight {
        height: 100%;
    }</style><style type="text/css">.ViewSDK_viewportRelativeHeight {
        height: 100vh;
    }</style><style type="text/css">.ViewSDK_fullScreenPDFViewer {
        width:100vw !important; height:100vh !important;
        max-width:100vw !important; max-height:100vh !important;
        position:fixed; top:0; left:0; z-index:9999;
        margin: 0 !important; padding: 0 !important;
        border: none !important;
    }</style><style type="text/css">.ViewSDK_LBFullScreenPDFViewer {
        width:100vw !important; height:100vh !important;
        max-width:100vw !important; max-height:100vh !important;
        position:fixed; top:0; left:0; z-index:10000;
        margin: 0 !important; padding: 0 !important;
        background-color: rgba(0, 0, 0, 0.2);
        border: none !important;
    }</style><style type="text/css">:root { --ViewSDK-mobile-viewport-height:1vh; }</style><style type="text/css">.ViewSDK_fullScreenPDFViewerMobile {
        width:100vw !important; height:calc(100 * var(--ViewSDK-mobile-viewport-height)) !important;
        max-width:100vw !important; max-height:calc(100 * var(--ViewSDK-mobile-viewport-height)) !important;
        position:fixed; top:0; left:0; z-index:9999;
        margin: 0 !important; padding: 0 !important;
        border: none !important;
    }</style><style type="text/css">.ViewSDK_LBLoader {
        position: absolute;
        left: 50%;
        top: 50%;
        z-index: 10000;
        border: 4px solid #f3f3f3;
        border-top: 4px solid #1473E6;
        width: 56px !important;
        height: 56px !important;
        margin: -32px 0 0 -32px;
        border-radius: 50%;
        -webkit-animation: lbSpin 2s linear infinite;
        animation: lbSpin 2s linear infinite;
    }
    @-webkit-keyframes lbSpin {
        0% { -webkit-transform: rotate(0deg); }
        100% { -webkit-transform: rotate(360deg); }
    }
    @keyframes lbSpin {
        0% { transform: rotate(0deg); }
        100% { transform: rotate(360deg); }
    }</style></head>
<!-- Head End -->

<!-- Body Start -->
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop is-max-mobile">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="font-size: 30pt;">
              Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT
            </h1>

            <!-- <div class="is-size-4 publication-authors">
              <span class="eql-cntrb">
                    <strong>NeurIPS 2025</strong>
                  <br>
                  <br>
              </span>
            </div> -->

            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Zhuobai Dong</a><sup>1 *</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Junchao Yi</a><sup>2 *</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Ziyuan Zheng</a><sup>1</sup>,</span>
                      <span class="author-block">
                        <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Haochen Han</a><sup>3</sup>,</span>
                          <span class="author-block">
                            <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Xiangxi Zheng</a><sup>4</sup>,</span>
                             <br>
                              <span class="author-block">
                                <a href="https://fingerrec.github.io/" target="_blank">Alex Jinpeng Wang</a><sup>1 &dagger;</sup>,</span>
                                  <span class="author-block">
                                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Fangming Liu</a><sup>3</sup>,</span>
                                      <span class="author-block">
                                        <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Linjie Li</a><sup>5</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Central South University,&nbsp;&nbsp;<sup>2</sup>University of Electronic Science and Technology of China<br><sup>3</sup>Peng Cheng Laboratory,&nbsp;&nbsp;<sup>4</sup>Nanjing University,&nbsp;&nbsp;<sup>5</sup>Microsoft</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution,&nbsp;<sup>&dagger;</sup>Corresponding author</small></span>
            </div>

            &nbsp;

            <!-- Logos -->
            <div class="logos">
              <img src="static/images/csu.jpg" alt="csu Logo" class="csu-logo" width="100">
              &nbsp;
              <img src="static/images/dianzi.png" alt="dianzi Logo" class="dianzi-logo" width="95">
              &nbsp;
              <img src="static/images/pengcheng.jpg" alt="pengcheng Logo" class="pengcheng" width="100">
              &nbsp;
              <img src="static/images/nanjing.png" alt="nanjing Logo" class="nanjing" width="72">
              &nbsp;
              <img src="static/images/microsoft.jpg" alt="microsoft Logo" class="microsoft" width="150">
            </div>

            <div class="column has-text-centered">
            <div class="publication-links">


            <!-- Arxiv PDF link -->
            <span class="link-block">
            <a href="https://arxiv.org/abs" class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
                <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><!-- <i class="fas fa-file-pdf"></i> Font Awesome fontawesome.com -->
            </span>
            <span>Paper</span>
            </a>
            </span>
            
            &nbsp;

            <!-- Github link -->
            <span class="link-block">
              <a href="https://github.com/CSU-JPG/MVPBench" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
              </span>
              <span>Code</span>
              </a>
              </span>
            
            &nbsp;
            
            <!-- Hugging Face PNG Button -->
            <span class="link-block">
              <a href="https://huggingface.co/datasets/CSU-JPG/MVPBench" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <img src="static/images/huggingface.png" alt="Hugging Face" style="height: 1em; vertical-align: middle;">
                </span>
                <span>Data</span>
              </a>
            </span>


            
            <!-- Hugging Face PDF link -->
            <!-- <span class="link-block">
              <a href="https://arxiv.org/abs" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 256" width="1em" height="1em">
                    <circle cx="128" cy="128" r="120" fill="#FFD21F"/>
                    <path d="M80 160a16 16 0 0 0 32 0h32a16 16 0 0 0 32 0" fill="#000"/>
                    <circle cx="96" cy="112" r="12" fill="#000"/>
                    <circle cx="160" cy="112" r="12" fill="#000"/>
                  </svg>
                </span>
                <span>Data</span>
              </a>
            </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="subtitle has-text-centered" style="margin-top: 10pt; font-size: 20pt; color: #20b30c; font-weight: 1000;">
      ▶️ Overview Video
      </div>
      <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
       
        <source src="./video/Omni-RGPT-video-v3.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image-container">
        <div class="zoomcaption">🔍 Click to zoom in</div>
        <img class="clickable-image" src="./static/images/figure1.png">
        <figcaption><br><b>Figure 1: A one-minute sanity check shatters the illusion <em>of spatial reasoning in MLLMs</em>.</b> 
          Red arrows indicate objects and multiple reasoning chains are provided to capture diverse yet valid solution strategies.
        </figcaption>
      </figure>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper Summary -->
<section class="section hero is-light">
  <div class="container is-max-desktop is-max-mobile">
  <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
  <div class="content has-text-justified">
    <strong>Abstract:</strong>
    Understanding the physical world—governed by laws of motion, spatial relations, and causality—poses a fundamental challenge for multimodal large language models (MLLMs).
    While recent advances such as OpenAI o3 and GPT-4o demonstrate impressive perceptual and reasoning capabilities, our investigation reveals these models struggle profoundly with visual physical reasoning, failing to grasp basic physical laws, spatial interactions, and causal effects in complex scenes. More importantly, they often fail to follow coherent reasoning chains grounded in visual evidence, especially when multiple steps are needed to arrive at the correct answer.
    To rigorously evaluate this capability, we introduce <b>MVPBench</b>, a curated benchmark designed to rigorously evaluate visual physical reasoning through the lens of visual chain-of-thought (CoT). 
    Each of the 1,211 examples features interleaved multi-image inputs and demands not only the correct final answer but also a coherent, step-by-step reasoning path grounded in evolving visual cues. 
    Each example features interleaved multi-image inputs and demands not only the correct final answer but also a coherent, step-by-step reasoning path grounded in evolving visual cues. 
    This setup mirrors how humans reason through real-world physical processes over time.
    To ensure fine-grained evaluation, we introduce a <b>graph-based CoT consistency metric</b> that verifies whether the reasoning path of model adheres to valid physical logic.
    Additionally, we minimize shortcut exploitation from text priors, encouraging models to rely on visual understanding.
    Experimental results reveal a concerning trend: even cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text alignment in physical domains. 
    Surprisingly, <b>RL-based post-training alignment—commonly believed to improve visual reasoning performance—often harms spatial reasoning</b>, suggesting a need to rethink current fine-tuning practices.
  </div>
  </div>
  </div>
  </div>
  </section>

<!-- Image with caption -->
<section class="section hero is-small">
<div class="container is-max-desktop is-max-mobile">
<figure class="image-container">
  <div class="zoomcaption">Dataset Overview</div>
  <img class="static-image" src="./static/images/dataset.png">
  <figcaption><br> <b>Figure 2: Examples from MVPBench across four major categories.</b> 
    Each example includes an initial scene followed by multiple reasoning steps. Target objects are marked with red arrows and labeled with letters to reduce textual bias.
  </figcaption>
</figure>
</div>
</section>

<!-- Slideshow container -->
<section class="section hero is-small">
  <div class="container is-max-desktop is-max-mobile">
  <link rel="stylesheet" href="./static/css/slider.css">
  <div class="zoomcaption">◀️ ▶️👆 Click to Navigate Qualitative Results</div>
  <div class="slider-container">
    <button class="slider-button left-button">❮</button>
    <img id="slider-image" src="static/images/relevance.png">
    <button class="slider-button right-button">❯</button>
  </div>
  </div>
</section>

<!-- Image with caption -->
<section class="section hero is-small">
<div class="container is-max-desktop is-max-mobile">
<figure class="image-container">
  <div class="zoomcaption">Performance comparison between single-image and multi-image inputs on CoT evaluation </div>
  <img class="static-image" src="./static/images/single_multi.png">
</figure>
</div>
</section>

<!-- Image with caption -->
<section class="section hero is-small">
<div class="container is-max-desktop is-max-mobile">
<figure class="image-container">
  <div class="zoomcaption">CoT Performance of MLLMs with post training versus without post-training </div>
  <img class="static-image" src="./static/images/post_train.png">
</figure>
</div>
</section>

<!-- <section class="section hero is-small">
  <div class="container is-max-desktop is-max-mobile">
  <figure class="image-container">
    <div class="zoomcaption">Quantitative Results (Image)</div>
    <img class="static-image-small" src="./Omni-RGPT_ Unifying Image and Video Region-level Understanding via Token Marks_files/vcr_comparison.png">
  </figure>
  </div>
  </section> -->



<!-- <section class="section hero is-small">
  <div class="container is-max-desktop is-max-mobile">
  <figure class="row-image-container">
    <div class="zoomcaption">RegVID-300k Dataset</div>
    <figcaption>We introduce a large-scale, diverse, and fine-grained <b>Reg</b>ion-level <b>Vi</b>deo <b>I</b>nstruction <b>D</b>ataset (<b>RegVID-300k</b>). 
    The dataset includes 98k unique videos, with 214k regions curated from 10 public video datasets and 294k region-level instruction samples. 
    </figcaption><br>
    <img class="static-image" src="./Omni-RGPT_ Unifying Image and Video Region-level Understanding via Token Marks_files/data-vis.jpg">
  </figure>
  </div>
</section> -->

<!-- Acknowledgements Section -->
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop is-max-mobile">
    <div class="columns is-centered">
      <div class="column is-three-quarters">
        <div class="content has-text-justified">
          <h2 class="title is-4 has-text-centered">Acknowledgements</h2>
          <p>
            We would like to thank Qiushan Guo for assistance in setting up the project. 
            We also appreciate the NVIDIA VILA team for the efforts in developing a robust framework that greatly facilitated our research.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- BibTeX Citation Section -->
<section class="section hero is-light" style="padding-top: 1rem; padding-bottom: 1rem;">
  <div class="container"> 
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content has-text-centered">
          <h2 class="title is-4">BibTeX</h2>
          <div class="bibtex-container" style="background-color: #f5f5f5; padding: 1.5rem; border-radius: 8px; display: inline-block; text-align: left; font-size: 1rem; overflow-x: auto; width: 100%; max-width: 800px; margin: 0 auto;">
            <pre class="is-family-monospace" style="margin: 0;">@article{dong2025mvpbench,
              title={Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT},
              author={Dong, Zhuobai and Yi, Junchao and Zheng, Ziyuan and Han, haochen and Zheng, Xiangxi and Wang, Alex Jinpeng and Liu, Fangming and Li, Linjie and others},
              year={2025}
            }
            </pre>
            <button onclick="copyBibtex()" style="margin-top: 1rem; background-color: #4CAF50; color: white; border: none; padding: 0.5rem 1rem; border-radius: 4px; cursor: pointer; font-size: 0.875rem;">Copy</button>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<script>
  function copyBibtex() {
    const bibtex = `@article{dong2025mvpbench,
  title={Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT},
  author={Dong, Zhuobai and Yi, Junchao and Zheng, Ziyuan and Han, haochen and Zheng, Xiangxi and Wang, Alex Jinpeng and Liu, Fangming and Li, Linjie and others},
  year={2025}
}`;
    navigator.clipboard.writeText(bibtex).then(() => {
      alert('BibTeX copied to clipboard!');
    });
  }
</script>


<!-- Citation for This Template -->
<footer class="footer">
<div class="container">
<div class="columns is-centered">
    <div class="column is-8">
    <div class="content">

        <p>
        This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the&nbsp;<a href="https://nerfies.github.io/" target="_blank">Nerfies</a>&nbsp;project page.
        You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>

    </div>
    </div>
</div>
</div>
</footer>


<!-- Java Script -->
<script>
  // Get all images with the class 'clickable-image'
  const images = document.querySelectorAll(".clickable-image");

  // Create a modal for the zoom effect
  const modal = document.createElement("div");
  modal.className = "fullscreen-modal";
  document.body.appendChild(modal);

  const closeButton = document.createElement("div");
  closeButton.className = "close-btn";
  closeButton.innerHTML = "&times;";
  modal.appendChild(closeButton);

  const fullscreenImage = document.createElement("img");
  modal.appendChild(fullscreenImage);

  // Add event listeners to all images
  images.forEach((image) => {
    image.addEventListener("click", () => {
      fullscreenImage.src = image.src; // Set the modal image to the clicked image
      modal.style.display = "flex";
    });
  });

  // Close the modal on close button click
  closeButton.addEventListener("click", () => {
    modal.style.display = "none";
  });

  // Close the modal on outside click
  modal.addEventListener("click", (e) => {
    if (e.target === modal) {
      modal.style.display = "none";
    }
  });



// slider
const image_list = [
    "static/images/relevance.png",
    "static/images/quality.png",
    "static/images/reflection.png",
    "static/images/diversity.png",
];

let currentIndex = 0;

const sliderImage = document.getElementById("slider-image");
const totalImages = image_list.length;

function showImage(index) {
    sliderImage.src = image_list[index]; 
}

function showNextImage() {
    currentIndex = (currentIndex + 1) % totalImages;
    showImage(currentIndex);
}

function showPreviousImage() {
    currentIndex = (currentIndex - 1 + totalImages) % totalImages;
    showImage(currentIndex);
}

document.querySelector(".left-button").addEventListener("click", showPreviousImage);
document.querySelector(".right-button").addEventListener("click", showNextImage);

document.addEventListener("keydown", (event) => {
    if (event.key === "ArrowLeft") {
        showPreviousImage();
    } else if (event.key === "ArrowRight") {
        showNextImage();
    }
});

</script><div class="fullscreen-modal"><div class="close-btn">×</div><img></div>


<!-- Body End -->


</body><!-- Html End --></html>